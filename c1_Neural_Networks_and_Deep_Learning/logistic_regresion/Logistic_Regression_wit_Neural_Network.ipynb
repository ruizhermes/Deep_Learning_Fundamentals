{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Making sense of data shape and size\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test =  test_set_x_orig.shape[0]\n",
    "num_px =  train_set_x_orig.shape[1]\n",
    "\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping (Flattening) training and test data sets \n",
    "# dimensions ==> (64x64x3,1) ==> (12288,1), where 12288 are the number of feautures\n",
    "\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_set_x_flatten =  test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 209)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarizing the data\n",
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1.0/(1.0+np.exp(-z))\n",
    "   \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "    \n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    A = sigmoid(np.dot(w.T,X)  + b)                                    # compute activation\n",
    "    cost = -1.0/m*(np.sum(Y*np.log(A) + ((1.0-Y)*np.log(1.0-A)  )))          # compute cost\n",
    "   \n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = 1.0/m * np.dot(X,(A - Y).T)\n",
    "    db = 1.0/m * np.sum(A - Y)\n",
    "   \n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.19033591]\n",
      " [0.12259159]]\n",
      "b = 1.9253598300845747\n",
      "dw = [[0.67752042]\n",
      " [1.41625495]]\n",
      "db = 0.21919450454067657\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together into a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.005, print_cost = True):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # initialize parameters with zeros \n",
    "    w, b = np.zeros((X_train.shape[0],1 )), 0\n",
    "\n",
    "    # Gradient descent \n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples \n",
    "    Y_prediction_test = sigmoid(np.dot(w.T, X_test) + b)\n",
    "    Y_prediction_train = sigmoid(np.dot(w.T, X_train) + b)\n",
    "   \n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.584508\n",
      "Cost after iteration 200: 0.466949\n",
      "Cost after iteration 300: 0.376007\n",
      "Cost after iteration 400: 0.331463\n",
      "Cost after iteration 500: 0.303273\n",
      "Cost after iteration 600: 0.279880\n",
      "Cost after iteration 700: 0.260042\n",
      "Cost after iteration 800: 0.242941\n",
      "Cost after iteration 900: 0.228004\n",
      "Cost after iteration 1000: 0.214820\n",
      "Cost after iteration 1100: 0.203078\n",
      "Cost after iteration 1200: 0.192544\n",
      "Cost after iteration 1300: 0.183033\n",
      "Cost after iteration 1400: 0.174399\n",
      "Cost after iteration 1500: 0.166521\n",
      "Cost after iteration 1600: 0.159305\n",
      "Cost after iteration 1700: 0.152667\n",
      "Cost after iteration 1800: 0.146542\n",
      "Cost after iteration 1900: 0.140872\n",
      "train accuracy: 88.072032072 %\n",
      "test accuracy: 64.8468117996 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
